{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0rjSR2x-bBv-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math, copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for the data of the housing prices\n",
        "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])\n",
        "\n",
        "# Input for the initial paramters for the GD\n",
        "b_init = 785.1811367994083\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
      ],
      "metadata": {
        "id": "1EYx3f78bUKl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization\n",
        "Simple Prediction Using vectorization and the originally initted values of weights and bias"
      ],
      "metadata": {
        "id": "M6OJ0nx4bhyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mLR(X, y, b, w):\n",
        "  \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      x (ndarray): Shape (n,) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "\n",
        "  pred = 0\n",
        "  m = X.shape[0] # m --> is the number of data points in the dataset\n",
        "  n = w.shape[0] # n --> is the number of features in the dataset\n",
        "  for i in range(m):\n",
        "    pred += np.dot(X[i], w) # This is so that every row will be multiplied\n",
        "  return (pred + b_init)"
      ],
      "metadata": {
        "id": "EsCV6-4-bfO-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_mLR(X_train, y_train, b_init, w_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImdgNuHxcMOW",
        "outputId": "48156d37-1ccc-44a8-c3dd-d7593b90add3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-700.362278620592"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Variate Cost Function\n",
        "<a name=\"toc_15456_4\"></a>\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
        "\n",
        "\n",
        "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
      ],
      "metadata": {
        "id": "-qf23ES7eMnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_compute(X, y, w, b):\n",
        "  \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "  m = X.shape[0]\n",
        "  cost = 0.0\n",
        "\n",
        "  for i in range(m):\n",
        "    f_wbi = np.dot(X[i], w) + b\n",
        "    cost += (f_wbi - y[i]) ** 2\n",
        "\n",
        "  return ((cost)/(2*m))"
      ],
      "metadata": {
        "id": "NHThpZRbeMHL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_compute(X_train, y_train, w_init, b_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f2lw94mf3a8",
        "outputId": "3ac2dc53-e0ae-4678-da65-f4f961684d38"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5578904428966628e-12"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Multi Variate Gradient\n",
        "<a name=\"toc_15456_5\"></a>\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "MauJXVjYhie2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_compute(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    n = w.shape[0]\n",
        "    dj_dw = np.zeros(n)\n",
        "    dj_db = 0.0\n",
        "\n",
        "    for i in range(m):\n",
        "      err = (np.dot(X[i], w) + b) - y[i]\n",
        "      for j in range(n):\n",
        "        dj_dw[j] += err * X[i, j]\n",
        "      dj_db += err\n",
        "\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return(dj_dw, dj_db)"
      ],
      "metadata": {
        "id": "F41g4lMFhmEg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute and display gradient\n",
        "tmp_dj_dw, tmp_dj_db = gradient_compute(X_train, y_train, w_init, b_init)\n",
        "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
        "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNfAXy9vjVEu",
        "outputId": "675adf09-6442-43e9-9a42-60b548e05049"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db at initial w,b: -1.6739251501955248e-06\n",
            "dj_dw at initial w,b: \n",
            " [-2.72623577e-03 -6.27197263e-06 -2.21745578e-06 -6.92403391e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[i]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "BTWxsrDQkF3N"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 1000\n",
        "alpha = 5.0e-7\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    cost_compute, gradient_compute,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "id": "TgzETLcNki6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}